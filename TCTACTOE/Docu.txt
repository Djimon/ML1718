At first the player looks, if it can set a mark on the central position of the 3D-field. 

Afterwards it creates a list of all boards which are possible after setting a mark on any empty field. For each board it then searches the field for lines of marks and sets variables of useful combinations of marks e.g. how many straight lines of 4 marks in a row are set on the field.  

These variables are then used in the evaluation function to calculate the "goodness" of the board. The best predicted draw is selected and will be executed. After the opponent's draw the player is using a training function to adapt the evaluation function with the knowledge of the results of the opponent's draw. 

Afterwards a new cycle of possible boards, evaluation, own draw, opponent's draw, training respectively learning begins until the match ends. 

 

The search for useful combinations is generally a depth-first search. Although it looks in all seven possible directions (looking backward on an indices is not made because all positions in the field are observed), it takes one direction and recursively follows the direction until no mark of the same player is found. Afterwards it takes the other directions one by one. After all seven directions it then uses the next position and starts all over.  

When the recursion gets into a blind end the variables for evaluation are incremented if no other mark is in this line. Otherwise this line wouldn't be of any use for this match. Because this (together with the condition that there are 5 fields in line) could be fulfilled to seldom for 2 to 4 marks in a row we mitigated the conditions, so that for value 2 and 3 only one neighboring field needs to be empty, which also doesn't test anymore if this line segment can be 5 marks long which isn't of any use for winning the game itself but might help the algorithm itself to improve. 

For training the weights we used the least mean square error algorithm showed in the lecture.

Against the RandomPlayer our algortihm performs quite well and the turns we need to win are decrease quickly (see diagrams). In a long term our training does not improve very much.

as a result of our learning we see, that our feature 7 has a high priority, whereas feature 8 becomes the lowest weights.
We did not reach the best possible result.